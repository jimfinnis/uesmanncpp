<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>UESMANN CPP: UESMANN CPP</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">UESMANN CPP
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">Reference implementation of UESMANN</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">UESMANN CPP </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This is the new C++ implementation of the UESMANN modulatory neural network architecture, based on the original code used in my thesis. See <a href="http://users.aber.ac.uk/jcf12/research/uesmann/">my University web site for more details and publications</a> or the brief introduction below.</p>
<h2>What is UESMANN?</h2>
<p>UESMANN is a very simple modification of a standard multilayer perceptron (MLP) with a logistic sigmoid activation function, trained using stochastic gradient descent. The modification consists of a modulatory factor <em>h</em> on the weights, such that the weights have their nominal values at <em>h</em>=0 and double those values at <em>h</em>=1. The biases are unmodulated.</p>
<p>As such, the network is able to perform multiple functions at different modulator levels, and is typically trained using examples of one function at <em>h</em>=0 and another at <em>h</em>=1, where it typically performs well. For example, a single network can be trained to perform any possible pairing of binary boolean functions in the same number of network parameters (weights and biases) required for a single such function. It has also been tested in <a class="el" href="classMNIST.html" title="This class encapsulates and loads data in the standard MNIST format. The data resides in two files...">MNIST</a> handwriting recognition and line recognition tasks, and in a homeostatic robot control problem.</p>
<p>A rather more complete set of documentation, including a description of the network and a Doxygen docs, can be found at</p>
<p><a href="https://jimfinnis.github.io/uesmanncpp/html/index.html">https://jimfinnis.github.io/uesmanncpp/html/index.html</a></p>
<h2>About the code</h2>
<p>The code itself is very simplistic, using scalar as opposed to matrix operations and no GPU acceleration. This is to make it as clear as possible, as befits a reference implementation, and also to match the implementation used in the thesis. There are no dependencies on any libraries beyond those found in a standard C++ install, and libboost-test for testing. You may find the code somewhat lacking in modern C++ style because I'm an 80's coder.</p>
<p>Implementations of the other network types mentioned in the thesis are also included:</p>
<ul>
<li>output blending (training two networks with identical architectures to perform the two different functions and using the modulator to linearly interpolate between their outputs);</li>
<li>h-as-input (applying the modulator as an extra input to a standard MLP and training accordingly)</li>
<li>plain (a straightforward MLP with no modifications)</li>
</ul>
<p>I originally intended to use Keras/Tensorflow, but would have been limited to using the low-level Tensorflow operations because of the somewhat peculiar nature of optimisation in UESMANN: we descend the gradient relative to the weights for one function, and the gradient relative to the weights times some constant for the other, alternating between the two. This makes the standard optimisers (such as ADAM) unsuitable. More investigation is planned, however, because a UESMANN layer may prove useful within a larger system. <b>If you can help with this, please let me know.</b></p>
<p><a href="https://travis-ci.com/jimfinnis/uesmanncpp"></a> </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
