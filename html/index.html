<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>UESMANN CPP: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">UESMANN CPP
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">Reference implementation of UESMANN</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Main Page </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>This is the new C++ implementation of UESMANN, based on the original code used in my thesis. That code was written as a <code>.angso</code> library for the Angort language, and has evolved to become rather unwieldly (as well as being intended for use from a language no-one but me uses).</p>
<p>The code is very simplistic, using scalar as opposed to matrix operations and no GPU acceleration. This is to make the code as clear as possible, as befits a reference implementation, and also to match the implementation used in the thesis. There are no dependencies on any libraries beyond those found in a standard C++ install, and libboost-test for testing. You may find the code somewhat lacking in modern C++ style because I'm an 80's coder.</p>
<p>I originally intended to write use Keras/Tensorflow, but would have been limited to using the low-level Tensorflow operations because of the somewhat peculiar nature of optimisation in UESMANN: we descend the gradient relative to the weights for one function, and the gradient relative to the weights times some constant for the other. A Keras/Tensorflow implementation is planned.</p>
<p>Implementations of the other network types mentioned in the thesis are also included.</p>
<p>The top level class is <a class="el" href="classNet.html">Net</a>, which is an virtual type describing the neural net interface and performing some basic operations. Other classes are:</p>
<ul>
<li><a class="el" href="classNetFactory.html" title="This class - really a namespace - contains functions which create, load or save networks of all types...">NetFactory</a>, which creates, loads and saves these concrete subclasses of <a class="el" href="classNet.html" title="The abstract network type upon which all others are based. It&#39;s not pure virtual, in that it encapsul...">Net</a>:<ul>
<li><a class="el" href="classBPNet.html" title="The &quot;basic&quot; back-propagation network using a logistic sigmoid, as described by Rumelhart, Hinton and Williams (and many others). This class is used by output blending and h-as-input networks. ">BPNet</a>, a plain, unmodulated MLP trained with backprop</li>
<li><a class="el" href="classOutputBlendingNet.html" title="A modulatory network architecture which uses two plain backprop networks, each of which is trained se...">OutputBlendingNet</a> and <a class="el" href="classHInputNet.html" title="A modulatory network architecture which uses a plain backprop network with an extra input to carry th...">HInputNet</a>, which are alternative modulatory networks described below</li>
<li><a class="el" href="classUESNet.html" title="The UESMANN network, which it itself based on the BPNet code as it has the same architecture as the p...">UESNet</a>, the UESMANN network</li>
</ul>
</li>
<li><a class="el" href="classExampleSet.html" title="A set of example data. Each datum consists of hormone (i.e. modulator value), inputs and outputs...">ExampleSet</a>, which is a set of examples for training networks</li>
<li><a class="el" href="structNet_1_1SGDParams.html" title="Training parameters for trainSGD(). This structure holds the parameters for the trainSGD() method...">Net::SGDParams</a>, which controls how training is performed (including crossvalidation and hyperparameters)</li>
<li><a class="el" href="classMNIST.html" title="This class encapsulates and loads data in the standard MNIST format. The data resides in two files...">MNIST</a>, which encapsulates MNIST-format data sets and from which <a class="el" href="classExampleSet.html" title="A set of example data. Each datum consists of hormone (i.e. modulator value), inputs and outputs...">ExampleSet</a> instances can be generated.</li>
</ul>
<h2>The library and test executables</h2>
<p>The entire library is include-only, just include <a class="el" href="netFactory_8hpp.html" title="I&#39;m not a fan of factories, but here&#39;s one - this makes a network of the appropriate type which confo...">netFactory.hpp</a> to get everything. The provided CMakeLists.txt builds two executables:</p>
<ul>
<li><b>uesmann-test</b> runs a set of test suites written using the Boost test framework.</li>
<li><b>genBoolGrid</b> trains 1000 UESMANN networks for every possible binary boolean function pairing and calculates how many perform the required function (this test was designed to ensure that the library's output matched that from equivalent code used in the thesis).</li>
</ul>
<h2>The network</h2>
<p>The network implemented is a modified version of the basic Rumelhart, Hinton and Williams multilayer perceptron with a logistic sigmoid activation function, and is trained using stochastic gradient descent by the back-propagation of errors. The modification consists of a modulatory factor <img class="formulaInl" alt="$h$" src="form_8.png"/>, which essentially doubles all the weights at 1, while leaving them at their nominal values at 0. Each node has the following function:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ y = \sigma\left(b+(h+1)\sum_i w_i x_i\right), \]" src="form_9.png"/>
</p>
<p>where</p>
<ul>
<li><img class="formulaInl" alt="$y$" src="form_10.png"/> is the node output,</li>
<li><img class="formulaInl" alt="$\sigma$" src="form_11.png"/> is the "standard" logistic sigmoid activation <img class="formulaInl" alt="$\sigma(x) = \frac{1}{1+e^{-x}}$" src="form_12.png"/></li>
<li><img class="formulaInl" alt="$b$" src="form_13.png"/> is the node bias,</li>
<li><img class="formulaInl" alt="$h$" src="form_8.png"/> is the modulator,</li>
<li><img class="formulaInl" alt="$w_i$" src="form_14.png"/> are the node weights,</li>
<li><img class="formulaInl" alt="$x_i$" src="form_15.png"/> are the node inputs.</li>
</ul>
<p>The network is trained to perform different functions at different modulator levels, currently two different functions at h=0 and h=1. This is done by modifying the back-propagation equations to find the gradient <img class="formulaInl" alt="$\frac{\partial C}{\partial w_i(1+h) }$" src="form_16.png"/> (i.e. the cost gradient with respect to the modulated weight). Each example is tagged with the appropriate modulator value, and the network is trained by presenting examples for alternating modulator levels so that the mean gradient followed will find a compromise between the solutions at the two levels.</p>
<p>It works surprisingly well, and shows interesting transition behaviour as the modulator changes. In the thesis, it has been tested on:</p>
<ul>
<li>boolean functions: all possible pairings of binary boolean functions can be performed in the same parameter space as a single boolean function (i.e. two hidden nodes);</li>
<li>image classification: it performs well on line recognition tasks, modulating between horizontal and vertical line recognition in noisy images; and on the <a class="el" href="classMNIST.html" title="This class encapsulates and loads data in the standard MNIST format. The data resides in two files...">MNIST</a> handwriting recognition task, modulating between recognising digits with their nominal labelling and an alternate labelling with a maximal Hamming distance;</li>
<li>robot control: in a homeostatic robot task, a real robot transitioned between exploration and phototaxis as simulated battery charge (which provided the modulator) fell.</li>
</ul>
<p>The performance of UESMANN was tested against:</p>
<ul>
<li>output blending: performing modulation by training two networks, one for each modulation level, and interpolating between the outputs using the modulator</li>
<li>h-as-input: providing the the modulator h as as extra input to a plain MLP and training accordingly.</li>
</ul>
<p>No enhancements of any kind were used in either UESMANN or the other networks to provide a baseline performance with no confounding factors. This means no regularisation, no adaptive learning rate, not even momentum (Nesterov or otherwise).</p>
<h2>Examples</h2>
<p>The files named <b>test..</b> have various unit tests in them which can be useful examples. The <a class="el" href="tests.html">Tests</a> page shows some of these tests in more detail. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
